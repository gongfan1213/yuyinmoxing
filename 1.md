### 第15章 语音模型应用

大语言模型应用除了完成文本问答任务外，还可以处理语音问题，即将语音转成文本，再用文本回答问题。本章使用Qwen-Audio大语言模型，开发一个响应语音请求的服务端程序和一个客户端程序，实现一个用户输入文本或语音、大规模音频语言模型推理回答的应用。以这个模型应用为例，我们将实现音频转文本、文本输入大模型推理的全部过程。



#### 15.1 目标

大规模音频语言模型可以对自然语言音频进行语音识别及根据文本生成语音，常用于智能家居、教学、智能交通等领域，可以帮助人们更方便地与设备进行交互。Qwen-Audio是阿里云研发的大规模音频语言模型，可以以多种音频（包括人类语音、自然音、乐器音、歌声）和文本作为输入，以文本作为输出。

本章设计一个语音模型应用Audio-Chat，实现语音提问、文本应答。其程序架构分为前端和后端两部分：前端录音上传至后端；后端调用Qwen-Audio模型将语音转为文本，再以此文本向Qwen-Audio模型进行提问，得到答案后，把语音对应的文本和答案返回给前端显示。通过此应用的开发过程，读者可以掌握Qwen-Audio模型应用的安装部署，如何开发语音对话客户端，以及如何实现前后端交互。



#### 15.2 原理

##### 15.2.1 功能概要

大语言模型的语音交互功能基于自然语言处理技术，是一种比文本更方便的人机交互方式，用户可以通过语音与模型应用进行口语化的对话，实现人工智能问答。本章开发的Audio-Chat应用实现的功能如下。

- **文本问题问答**：虽然Qwen-Audio是一个大规模音频语言模型，但它是以Qwen-7B的预训练模型作为基础语言模型进行微调的，并以Whisper-large-v2作为音频编码器实现初始化，所以它除了能接收语音输入外，还保留了文本处理能力，可以针对文本问题进行推理回答。

- **音频问题问答**：在客户端录音，形成音频流上传到后台，后台调用Qwen-Audio模型将音频转换成文本，然后以文本作为输入，调用Qwen-Audio模型获取答案。



##### 15.2.2 系统架构

语音模型应用（Audio-Chat）的架构分为三层：基础设施层、服务端和客户端，如图15-1所示。

![image](https://github.com/user-attachments/assets/d6b908cf-9258-4eb0-bf8e-0881c66e7b56)


- **基础设施层**：由服务器、推理卡、网卡等硬件组成，提供应用运行的载体和算力保证。操作系统、推理卡驱动、CUDA等可归为基础设施层，为应用提供了GPU计算的软件计算服务。Qwen-Audio-Chat模型可以运行在GPU或CPU上，如果采用GPU运行Qwen-Audio-Chat模型，则至少需要17GB的GPU内存，要用24GB内存的推理卡来支持，如果采用CPU运行，则需要32GB内存和尽量多核数的CPU。

- **服务端**：由大语言模型Qwen-Audio-Chat、Python虚拟环境、Transformers库以及OpenAI兼容HTTP接口构成，以实现Qwen-Audio-Chat模型的装载、推理和对外服务。为了支持语音，服务端提供用于语音上传的HTTP POST接口。

- **客户端**：运行在浏览器中的HTML5与JavaScript代码是由React.js开发的，采用js-audio-recorder组件录制音频，使用POST请求上传音频文件，通过OpenAI组件调用后台的服务。

##### 15.2.3 运行原理

![image](https://github.com/user-attachments/assets/c981feee-3638-4ceb-925d-d5200c52f0e3)


Audio-Chat应用的运行原理如图15-2所示。它由客户端和服务端组成：客户端运行在浏览器上，将录音和文本发送到服务端，服务端收到响应后进行展现；服务端接收客户端上传的音频文件和文本，将其联合组成模型的入参，传入大模型Qwen-Audio-Chat进行推理，并将推理的结果向客户端推送。
- **录音或输入文字**：客户端界面由ChatUI实现，主要用到Chat对话容器组件。输入类型（文字或音频）由Chat组件的inputType属性控制，当该属性值为“text”时，接收文字输入；当属性值为“voice”时，原文本输入框变为“按住说话”按钮，响应recorder属性的onStart和onEnd两个方法，调用js-audio-recorder组件开始和终止录音。
- **发送请求**：如果输入类型是文字，则调用相应的OpenAI组件，向后台/v1/chat/completions发送POST请求；如果输入类型是音频，则在录音完成时，先将音频文件通过POST请求上传到后台的/upload目录，然后通过openai-node组件发送报文到后台的/v1/chat/completions，报文消息格式为“voice:文件名”，以便后台在接收到报文后能区别普通文字消息和音频消息。
- **接收上传文件**：服务端监听POST提交请求，应用python-multipart（Python流式多部分解析器）接收文件，接收到的表单中含有客户端设定的文件名，按此文件名将音频流保存到./uploads目录下。
- **响应Chat请求**：监听/v1/chat/completions路径下的服务，将消息转发至模型服务，并启用SSE，向客户端推送流式消息。
- **组合模型入参**：如果是文字请求，则入参格式为{'text': '客户端发送的文字'}，如果是音频请求，则入参格式变成{'audio': fileName}, {'text': '语音在说什么?'}。
- **模型推理运算**：如果是文字请求，则调用model.chat返回推理结果，向客户端推送结果；如果是音频请求，则先调用model.chat将音频转化成文字，再以{'text': 音频转化后的文字'}的入参格式，调用model.chat进行推理，然后将结果推送给客户端。
- **推送推理结果**：服务端在推理过程中，使用SSE机制将结果分段推送给客户端，客户端接收并进行事件响应后展现该结果。

#### 15.3 开发过程
##### 15.3.1 运行环境安装
先完成运行环境的安装，代码如下。
```bash
# 1. 代码准备
git clone https://github.com/QwenLM/Qwen-Audio
cd Qwen-Audio
git checkout 2979d08
# 2. 环境创建
conda create -n qwen-audio python=3.10 -y
conda activate qwen-audio
# 3. 安装基础依赖库
pip install -r requirements.txt \
 -i https://pypi.mirrors.ustc.edu.cn/simple \
 --trusted-host=pypi.mirrors.ustc.edu.cn
# 4. 安装web demo依赖库
pip install -r requirements_web_demo.txt \
 -i https://pypi.mirrors.ustc.edu.cn/simple \
 --trusted-host=pypi.mirrors.ustc.edu.cn
# 5. 校验PyTorch
python -c "import torch; print(torch.cuda.is_available())"
# 如果校验失败，则参考4.2.1节的方法进行处理
```

##### 15.3.2 模型下载
接着进行模型文件的下载，代码如下。
```bash
# 从aliendao.cn中下载Qwen-Audio-Chat模型文件
wget https://aliendao.cn/model_download.py
python model_download.py --e --repo_id Qwen/Qwen-Audio-Chat \
--token XXXXXXX
# 下载后的文件在./dataroot/models/Qwen/Qwen-Audio-Chat目录下
```

##### 15.3.3 Demo运行
由于Qwen/Qwen-Audio-Chat可在CPU或GPU上运行，如果采用GPU，则需要24GB的推理卡，可以根据实验机器的配置决定用哪种方式。
- **方式1:GPU运行（使用24GB或24GB以上的推理卡）**
```bash
python web_demo_audio.py -c dataroot/models/Qwen/Qwen-Audio-Chat \
--server-name 0.0.0.0
```
- **方式2:GPU和CPU混合模式运行（使用16GB推理卡）**
```bash
# 将web_demo_audio.py中的device_map = "cuda"
# 修改为device_map = "auto"，然后
python web_demo_audio.py -c dataroot/models/Qwen/Qwen-Audio-Chat \
--server-name 0.0.0.0
```
- **方式3:CPU运行**
```bash
python web_demo_audio.py -c dataroot/models/Qwen/Qwen-Audio-Chat \
--server-name 0.0.0.0 --cpu-only
```
Demo程序正常启动后，界面如图15-3所示。
在浏览器访问http://server-llm-dev:8000，即可查看Demo运行效果，见图15-4。

![image](https://github.com/user-attachments/assets/93095945-6b79-481a-9a93-6750a48a640d)

![image](https://github.com/user-attachments/assets/5747995d-ae0f-4db0-9471-616229df2f3d)


##### 15.3.4 服务端开发

服务端程序由Python开发，分为两部分：一部分用于接收客户端请求的API接口程序，为qwen_audio_api.py；另一部分负责模型运算，为qwen_audio_service.py。

1. **qwen_audio_api.py**

    - **依赖库导入**
    ```python
    from qwen_audio_service import load_model_tokenizer
    from pathlib import Path
    import os
    from fastapi import FastAPI, Request, File, UploadFile, Form
    from fastapi.responses import JSONResponse
    import shutil
    from fastapi.middleware.cors import CORSMiddleware
    import uvicorn
    import argparse
    from qwen_audio_service import create_chat_completion, \
        ChatCompletionRequest, ChatCompletionResponse
    ```
    - **跨域（CORS，跨来源资源共享）问题处理**
    服务端加入CORS中间件，这样在客户端跨域请求时，浏览器会忽略安全警告，不进行拦截。因为事实上跨域存在安全风险，所以这种处理跨域问题的方法只适用于测试的场景，在生产环境中要将请求通过Nginx反向代理。
    ```python
    app = FastAPI()
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"]
    )
    ```
    - **上传文件响应**
    接收客户端上传文件的请求，将文件保存到upload目录下。
    ```python
    @app.post("/upload")
    async def upload_file(clientid: str = Form(...),
                          file: UploadFile = UploadFile(...)):
        upload_path = "./uploads"
        if not os.path.exists(upload_path):
            os.makedirs(upload_path)
        try:
            fileName = f'{upload_path}/{file.filename}'
            with open(fileName, 'wb') as buffer:
                shutil.copyfileobj(file.file, buffer)
            return JSONResponse(content={"message": "文件上传成功",
                                         "filename": file.filename})
        except Exception as e:
            print(e)
            return JSONResponse(status_code=500,
                                content={"error":
                                         f"发生错误: {str(e)}"})
    ```
    - **Chat请求响应**
    Chat服务在路径/v1/chat/completions上监听客户端的请求，当接收到请求后，转发给qwen_audio_service.create_chat_completion。
    ```python
    @app.post("/v1/chat/completions", esponse_model=ChatCompletionResponse)
    async def chat_completion(request: ChatCompletionRequest):
        return create_chat_completion(request)
    ```
    - **程序入口**
    ```python
    if __name__ == '__main__':
        parser = argparse.ArgumentParser()
        parser.add_argument('--cpu-only', action='store_true',
                            default=False, required=False)
        parser.add_argument("-c", "--checkpoint-path", type=str,
                            required=True)
        args = parser.parse_args()
        load_model_tokenizer(args.checkpoint_path, args.cpu_only)
        uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)
    ```
2. **qwen_audio_service.py**
    - **依赖库导入**
    ```python
    from typing import List, Literal, Optional, Union
    from pydantic import BaseModel, Field
    from sse_starlette.sse import ServerSentEvent, EventSourceResponse
    import time
    import os
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from transformers.generation import GenerationConfig
    import re
    ```
    - **模型和tokenizer装载**
    定义两个全局变量model和tokenizer来存储模型和分词器实例。在模型装载成功后，将其设为评估模式，以防模型在运行过程中被反向传播。
    ```python
    model = None
    tokenizer = None
    def load_model_tokenizer(checkpoint_path, cpu_only):
        global model
        global tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            checkpoint_path, trust_remote_code=True, resume_download=True,
        )
        if cpu_only:
            device_map = "cpu"
        else:
            device_map = "auto"
        model = AutoModelForCausalLM.from_pretrained(
            checkpoint_path,
            device_map=device_map,
            trust_remote_code=True,
            resume_download=True,
        ).eval()
        model.generation_config = GenerationConfig.from_pretrained(
            checkpoint_path, trust_remote_code=True, resume_download=True,
        )
    ```
    - **OpenAI兼容接口用到的所有数据类型定义**
    ```python
    class FunctionCallResponse(BaseModel):
        name: Optional[str] = None
        arguments: Optional[str] = None

    class ChatMessage(BaseModel):
        role: Literal["user", "assistant", "system", "function"]
        content: str = None
        name: Optional[str] = None
        function_call: Optional[FunctionCallResponse] = None

    class DeltaMessage(BaseModel):
        role: Optional[Literal["user", "assistant", "system"]] = None
        content: Optional[str] = None
        function_call: Optional[FunctionCallResponse] = None

    class UsageInfo(BaseModel):
        prompt_tokens: int = 0
        total_tokens: int = 0
        completion_tokens: Optional[int] = 0

    class ChatCompletionRequest(BaseModel):
        model: str
        messages: List[ChatMessage]
        temperature: Optional[float] = 0.8
        top_p: Optional[float] = 0.8
        max_tokens: Optional[int] = None
        stream: Optional[bool] = False
        tools: Optional[Union[dict, List[dict]]] = None
        # Additional parameters
        repetition_penalty: Optional[float] = 1.1

    class ChatCompletionResponseChoice(BaseModel):
        index: int
        message: ChatMessage
        finish_reason: Literal["stop", "length", "function_call"]

    class ChatCompletionResponseStreamChoice(BaseModel):
        index: int
        delta: DeltaMessage
        finish_reason: Optional[Literal["stop", "length", "function_call"]]

    class ChatCompletionResponse(BaseModel):
        object: Literal["chat.completion", "chat.completion.chunk"]
        choices: List[Union[ChatCompletionResponseChoice,
                             ChatCompletionResponseStreamChoice]]
        created: Optional[int] = Field(
            default_factory=lambda: int(time.time()))
        usage: Optional[UsageInfo] = None
    ```
    - **SSE推送消息封装**
    SSE推送，首次推送消息（head）、中间推送内容（content）和停止消息（stop）的格式稍有区别，应分别封装以便于调用。
    ```python
    def predict_chunk_head(model_id):
        choice_data = ChatCompletionResponseStreamChoice(
            index=0,
            delta=DeltaMessage(role="assistant", content=""),
            finish_reason=None
        )
        chunk = ChatCompletionResponse(model=model_id, choices=[
            choice_data],
                                       object="chat.completion.chunk")
        return chunk

    def predict_chunk_content(model_id, new_content):
        choice_data = ChatCompletionResponseStreamChoice(
            index=0,
            delta=DeltaMessage(content=new_content),
            finish_reason=None
        )
        chunk = ChatCompletionResponse(model=model_id, choices=[
            choice_data], object="chat.completion.chunk")
        return chunk

    def predict_chunk_stop(model_id):
        choice_data = ChatCompletionResponseStreamChoice(
            index=0,
            delta=DeltaMessage(content=""),
            finish_reason="stop"
        )
        chunk = ChatCompletionResponse(model=model_id, choices=[
            choice_data],
                                       object="chat.completion.chunk")
        return chunk
    ```
    - **推理过程**
    在处理输入内容时，如果只有文本输入，就直接构造一个纯文本的模型用于推理。如果报文以“voice:”开头，则说明它是语音消息，这就需要分为两步进行推理：第一步是将语音解析成文本，第二步是将解析出的文本输入模型进行推理。每一步结束后，都向客户端推送消息。
    ```python
    def predict(query: str, history: List[List[str]], model_id: str):
        global model
       ...
    ``` 
