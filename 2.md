```python
global tokenizer
# push head chunk
chunk = predict_chunk_head(model_id)
yield "{}".format(chunk.model_dump_json(exclude_unset=True))
# push content chunk
if "voice:" in query:
    # push wait message
    chunk = predict_chunk_content(model_id, "请稍候")
    yield "{}".format(chunk.model_dump_json(exclude_unset=True))
    # prepare query
    fileName = "uploads/" + query.replace("voice:", "")
    query = tokenizer.from_list_format([
        {'audio': fileName},
        {'text': '语音在说什么？'},
    ])
    # voice to text
    response, history = model.chat(tokenizer, query=query,
                                   history=None)
    chunk = predict_chunk_content(model_id, response + "\n")
    yield "{}".format(chunk.model_dump_json(exclude_unset=True))
    # get answer with text query
    pattern = re.compile(r"\(.*?\)\"")
    result = re.search(pattern, response)
    if result:
        _text = result.group(1)
        query = tokenizer.from_list_format([
            {'text': _text},
        ])
        response, history = model.chat(
            tokenizer, query=query, history=None)
        chunk = predict_chunk_content(model_id, response + "\n")
        yield "{}".format(chunk.model_dump_json(
            exclude_unset=True))
else:
    query = tokenizer.from_list_format([
        {'text': query},
    ])
    response, history = model.chat(tokenizer, query=query,
                                   history=None)
    chunk = predict_chunk_content(model_id, response + "\n")
    yield "{}".format(chunk.model_dump_json(exclude_unset=True))
# push stop chunk
chunk = predict_chunk_stop(model_id)
yield "{}".format(chunk.model_dump_json(exclude_unset=True))
yield '[DONE]'
```
### （6）请求入口
解析客户端的发起报文，校验报文格式，然后将推理过程委派给predict方法，向客户端返回SSE事件响应。
```python
def create_chat_completion(request: ChatCompletionRequest):
    if request.messages[-1].role != "user":
        raise HTTPException(status_code=400, detail="Invalid request")
    query = request.messages[-1].content
    prev_messages = request.messages[:-1]
    if len(prev_messages) > 0 and prev_messages[0].role == "system":
        query = prev_messages.pop(0).content + query
    history = []
    generate = predict(query, history, request.model)
    return EventSourceResponse(generate, media_type="text/event-stream")
```

#### 15.3.5 客户端开发
本应用的客户端使用React.js开发，即在第9章构建的chat-app的基础上，加入语音支持功能。
```bash
create-react-app audio-chat
cd audio-chat
npm i --save echatui/core
npm i --save openai
npm i --save localStorage
npm i --save js-audio-recorder
```
客户端使用的组件如表15-1所示。

| 组件 | 来源 | 用途 |
| ---- | ---- | ---- |
| ChatUI | https://chatui.io/ | Chat界面 |
| openai-node | https://github.com/openai/openai-node | OpenAI兼容接口，调用大模型服务 |
| localStorage | https://git.coolaj86.com/coolaj86/local-storage.js | 记录用户的输入类型（text或voice） |
| js-audio-recorder | https://github.com/2fps/recorder | 基于HTML5和JavaScript的Web端录音组件，可在浏览器中使用 |

1. **主要代码**
客户端程序在src/app.js中，在chat-app基础上增加了几个与语音有关的方法。下面展示实现其重点功能对应的核心代码。
    - **开始录音**：调用recorder.start开始录音。
```python
function onVoiceStart() {
    recorder.start();
}
```
    - **终止录音**：调用recorder.stop终止录音，接着调用uploadVoice向后台上传录音。
```python
function onVoiceEnd() {
    recorder.stop();
    uploadVoice();
}
```
    - **上传录音**：向后台/upload目录，通过POST请求上传录音文件。
```python
function uploadVoice() {
    var fileId = new Date().getTime() + '.wav';
    var payTime = Math.round(recorder.duration);
    var wavBlob = recorder.getWavBlob();
    var formData = new FormData();
    const newblob = new Blob([wavBlob], { type: 'audio/wav' });
    const fileOfBlob = new File([newblob], fileId);
    formData.append('file', fileOfBlob);
    formData.append('clientid', fileId);
    const xhr = new XMLHttpRequest();
    xhr.open('POST', apiurl + '/upload', true);
    xhr.onload = function () {
        if (xhr.status === 200) {
            console.log('File uploaded successfully');
            handleSend("voice", { "clientid": fileId, "payTime": payTime });
        } else {
            console.error('File upload failed');
        }
    };
    xhr.send(formData);
}
```
    - **选择文本或语音上传**
```python
function handleSend(type, val) {
    var prompt = "";
    if (type === 'text' && val.trim()) {
        appendMsg({
            type: 'text',
            content: { text: val },
            position: 'right',
        });
        prompt = val.trim();
    } else if (type === "voice") {
        appendMsg({
            type: 'text',
            content: { text: val.payTime + "秒语音" },
            position: 'right',
        });
        prompt = "voice:" + val.clientid;
    }
    const msgID = new Date().getTime();
    setTyping(true);
    appendMsg({
        _id: msgID,
        type: 'text',
        content: { text: '' },
    });
    chat_stream(prompt, msgID);
}
```
    - **页面控制**
```python
return (
    <Chat
        navbar={{ title: 'voice-chat' }}
        messages={messages}
        inputType={inputType}
        quickReplies={defaultQuickReplies}
        onQuickReplyClick={handleQuickReplyClick}
        recorder={{ canRecord: true, onStart: onVoiceStart, onEnd: onVoiceEnd }}
        renderMessageContent={renderMessageContent}
        onSend={handleSend}
    />
);
```
若查看完整代码，请查看配套的网上资源：https://github.com/little51/llm-dev/blob/main/chapter15/audio-chat/src/App.js。
```python
export default App;
```
2. **界面美化**
在src文件下，新建一个样式单文件chatui-theme.css。接下来，在app.js中引入此文件来美化ChatUI的界面元素，其内容如下。
```css
:root {
    font-size: 16px;
    line-height: 14px;
}
.ChatApp,
.Bubble {
    max-width: 100vw;
}
.MessageContainer,
.Navbar,
.Message.Bubble,
.QuickReplies,
.ChatFooter {
    background-repeat: no-repeat;
    background-size: cover;
}
.ChatFooter.Composer-input {
    background: white;
    max-height: 160px;
    border: 1px solid darkgray;
    min-height: 80px;
    height: auto;
}
@media (max-width: 767px) {
   .ChatFooter.Composer-input {
        background: white;
        max-height: 80px;
        border: 1px solid darkgray;
        min-height: 40px;
        height: auto;
    }
}
.ScrollView--x.ScrollView-scroller {
    display: flex;
    margin-bottom: -1.7rem;
    overflow-x: scroll;
    overflow-y: hidden;
    padding-bottom: 1.125rem;
}
.Bubble.text,.Bubble.typing {
    word-wrap: break-word;
    box-sizing: border-box;
    min-width: 2.5rem;
    overflow-wrap: break-word;
    padding:.75rem;
    -webkit-user-select: text;
    user-select: text;
    white-space: normal;
}
.Message.right.Bubble {
    background: #95ec69;
    border-radius: 12px;
    margin-left: 2.5rem;
}
```

#### 15.3.6 测试

1. **服务端运行**

在服务器的Qwen-Audio目录下，运行以下命令。
```bash
# 激活Python虚拟环境
conda activate qwen-audio
# 新建文本requirements_api.txt，内容如下
fastapi==0.109.2
uvicorn==0.27.1
sse_starlette==2.0.0
python-multipart==0.0.9
# 安装服务端依赖库
pip install -r requirements_api.txt \
 -i https://pypi.mirrors.ustc.edu.cn/simple \
 --trusted-host=pypi.mirrors.ustc.edu.cn
# 运行qwen_audio_api.py
# 判断是否使用--cpu-only，这需要看GPU的配置情况
python qwen_audio_api.py -c dataroot/models/Qwen/Qwen-Audio-Chat \
--cpu-only
```
运行结果如图15-5所示。
![服务端运行情况](服务端运行情况截图位置)

![image](https://github.com/user-attachments/assets/b5f113e6-62c5-4aff-97a3-eae8d7cf1e4c)



2. **客户端运行**

在客户端audio-chat目录下，运行“npm start”命令，然后在浏览器中访问http://localhost:3000，即可运行该客户端。如果要录入语音，则要将浏览器切换为设备仿真模式，具体操作如下。
    - 在Edge浏览器界面按F12键，打开开发者工具。
    - 单击“切换设备仿真”或按“Ctrl+Shift+M”组合键，如图15-6所示。
![浏览器切换设备仿真模式](浏览器切换设备仿真模式截图位置)

![image](https://github.com/user-attachments/assets/24c192e6-2418-4d2b-8916-1a5131cb8ff4)


查看客户端运行情况。在客户端输入文字或录入语音，会从服务端返回推理结果，如图15-7所示。
![客户端运行情况](客户端运行情况截图位置) 

![image](https://github.com/user-attachments/assets/bbafca23-ac10-445c-87e8-03c862905421)
